{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "mBERT-Training-with-validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwyIEdEg70Ch",
        "colab_type": "text"
      },
      "source": [
        "## TPU-8-CORES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMw55rtuaeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "outputId": "b4d89a73-dd34-437f-d534-5521eb832419"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4264  100  4264    0     0  56853      0 --:--:-- --:--:-- --:--:-- 56853\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Uninstalling torch-1.5.0+cu101:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.5.0+cu101\n",
            "Uninstalling torchvision-0.6.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (377 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSqBa5ro2_QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "4f62366c-dbe3-4ae4-a743-e4417c0ed6cb"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 15.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c47f1c3cb52e9bfe31132838943da2d5ab5d15fa3d0dc943dd371a3f607e4303\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTrcmWo3NCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "a5b4a741-0dcf-42b7-9820-c3224615c38b"
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjjV3Avue0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f84b9iC6ue4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to use TPU's using PyTorch, we have to use PyTorch-XLA library\n",
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLyyKPxAu2eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 224\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 2\n",
        "\n",
        "DIR_INPUT = '/content/gdrive/My Drive/Jigsaw/Toxic Comment'\n",
        "BERT_PATH = f'{DIR_INPUT}/input/bert_base_multilingual_uncased/' \n",
        "MODEL_PATH = f'{DIR_INPUT}/output/mBERT-Training-with-validation/model.bin' \n",
        "TRAINING_FILE_1 = f'{DIR_INPUT}/input/jigsaw-toxic-comment-train.csv' \n",
        "TRAINING_FILE_2 = f'{DIR_INPUT}/input/jigsaw-unintended-bias-train.csv' \n",
        "VALIDATION_FILE = f'{DIR_INPUT}/input/validation.csv' \n",
        "\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "    BERT_PATH,\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0RliQQ0wBM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDatasetTraining:\n",
        "  def __init__(self, comment_text, targets, tokenizer, max_length):\n",
        "    self.comment_text = comment_text\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.comment_text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    comment_text = str(self.comment_text[item])\n",
        "    comment_text = \" \".join(comment_text.split())\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        comment_text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_length,\n",
        "        pad_to_max_length = True\n",
        "    )\n",
        "\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    token_type_ids = inputs[\"token_type_ids\"]\n",
        "    mask = inputs[\"attention_mask\"]\n",
        "    \n",
        "    return {\n",
        "        'ids': torch.tensor(ids, dtype=torch.long),\n",
        "        'mask': torch.tensor(mask, dtype=torch.long),\n",
        "        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        'targets': torch.tensor(self.targets[item], dtype=torch.float)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "16a3df23-8416-46b5-8661-c52345005b6d",
        "_uuid": "993abe0b-2561-4abc-a6a2-b83d8dd4c846",
        "id": "wDk7ir2xttkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "  def __init__(self, bert_path):\n",
        "    super(BERTBaseUncased, self).__init__()\n",
        "    self.bert_path = bert_path\n",
        "    self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
        "    self.bert_drop = nn.Dropout(0.3)\n",
        "    self.out = nn.Linear(768 * 2, 1)\n",
        "\n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    o1, o2 = self.bert(ids,\n",
        "                       attention_mask=mask,\n",
        "                       token_type_ids=token_type_ids)\n",
        "    \n",
        "    apool = torch.mean(o1, 1)\n",
        "    mpool, _ = torch.max(o1, 1)\n",
        "    cat = torch.cat((apool, mpool), 1)\n",
        "\n",
        "    bo = self.bert_drop(cat)\n",
        "    output = self.out(bo)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSrCns1fyFYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "  return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYhs1cGHyFlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "  model.train()\n",
        "  for bi, d in enumerate(data_loader):\n",
        "    ids = d[\"ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    if bi % 10 == 0:\n",
        "        xm.master_print(f'bi={bi}, loss={loss}')\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    ####################################### CHANGE HAPPENS HERE #######################################################\n",
        "    xm.optimizer_step(optimizer)\n",
        "    ###################################################################################################################\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "and7u9ajyW-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_loop_fn(data_loader, model, device):\n",
        "  model.eval()\n",
        "  fin_targets = []\n",
        "  fin_outputs = []\n",
        "  for bi, d in enumerate(data_loader):\n",
        "    ids = d[\"ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "    outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    targets_np = targets.cpu().detach().numpy().tolist()\n",
        "    outputs_np = outputs.cpu().detach().numpy().tolist()\n",
        "    fin_targets.extend(targets_np)\n",
        "    fin_outputs.extend(outputs_np)    \n",
        "\n",
        "  return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M68znkRhGdiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTBaseUncased(bert_path=BERT_PATH)\n",
        "\n",
        "df1 = pd.read_csv(TRAINING_FILE_1, usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
        "df2 = pd.read_csv(TRAINING_FILE_2, usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
        "\n",
        "df_train = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
        "df_train = df_train.sample(frac=1).reset_index(drop=True).head(200000) \n",
        "\n",
        "df_valid = pd.read_csv(VALIDATION_FILE, usecols=[\"comment_text\", \"toxic\"])\n",
        "\n",
        "df_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\n",
        "df_train = df_train.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSbcrOjttk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _run():\n",
        "    \n",
        "  train_dataset = BERTDatasetTraining(\n",
        "      comment_text=df_train.comment_text.values,\n",
        "      targets=df_train.toxic.values,\n",
        "      tokenizer=TOKENIZER,\n",
        "      max_length=MAX_LEN\n",
        "  )\n",
        "\n",
        "  #-----------------------------#---------------------#-----------------------------------#-----------------------------------#\n",
        "  ##################################### Change occurs Here ####################################################################\n",
        "  # We have to use DistributedSampler to use TPU's. It will distribute the data on different TPU cores.\n",
        "  # DistributedSampler: Sampler that restricts data loading to a subset of the dataset.\n",
        "  # https://kite.com/python/docs/torch.utils.data.DistributedSampler\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True)\n",
        "\n",
        "  # DataLoader: Combines a dataset and a sampler, and provides an iterable over the given dataset.  \n",
        "  train_data_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=TRAIN_BATCH_SIZE,\n",
        "      sampler=train_sampler,\n",
        "      drop_last=True,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "  valid_dataset = BERTDatasetTraining(\n",
        "      comment_text=df_valid.comment_text.values,\n",
        "      targets=df_valid.toxic.values,\n",
        "      tokenizer=TOKENIZER,\n",
        "      max_length=MAX_LEN\n",
        "  )\n",
        "\n",
        "  valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        valid_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False)\n",
        "\n",
        "  valid_data_loader = torch.utils.data.DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size=VALID_BATCH_SIZE,\n",
        "      sampler=valid_sampler,\n",
        "      drop_last=False,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "  #-----------------------------#---------------------#-----------------------------------#-----------------------------------#\n",
        "  ##################################### Change occurs Here ####################################################################\n",
        "  device = xm.xla_device()\n",
        "  model.to(device)\n",
        "  #############################################################################################################################\n",
        "  #----------------------------#----------------------#------------------------------------#-----------------------------------#\n",
        "\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "  # Calculate the number of training steps\t\n",
        "  # Number of training steps will get divided my number of cores\n",
        "  lr = 0.4 * 1e-5 * xm.xrt_world_size()\n",
        "  num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
        "  xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_train_steps\n",
        "  )\n",
        " \n",
        "  best_roc_auc = 0\n",
        "  ########################################## Change occur In this Loop #################################################################\n",
        "  for epoch in range(EPOCHS):\n",
        "    # train_data_loader has to be wrapped inside ParallelLoader \n",
        "    para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n",
        "\n",
        "    para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "    o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n",
        "  ########################################################################################################################################  \n",
        "\n",
        "    roc_auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n",
        "    xm.master_print(f'AUC = {roc_auc}')\n",
        "    if roc_auc > best_roc_auc:\n",
        "      # Instead of using torch.save, we will be saving using xm.save\n",
        "      xm.save(model.state_dict(), MODEL_PATH)\n",
        "      best_roc_auc = roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDgSTc5lB19w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4e18768-7481-40fb-efbc-b7d2c7cbd1a0"
      },
      "source": [
        "# Start training processes\n",
        "def _multiprocessing_function(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = _run()\n",
        "\n",
        "FLAGS={}\n",
        "xmp.spawn(_multiprocessing_function, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train_steps = 1625, world_size=8\n",
            "bi=0, loss=0.7023844122886658\n",
            "bi=10, loss=0.2372722029685974\n",
            "bi=20, loss=0.4337332248687744\n",
            "bi=30, loss=0.36015355587005615\n",
            "bi=40, loss=0.2171345204114914\n",
            "bi=50, loss=0.25924423336982727\n",
            "bi=60, loss=0.25696200132369995\n",
            "bi=70, loss=0.24473780393600464\n",
            "bi=80, loss=0.36457887291908264\n",
            "bi=90, loss=0.34517255425453186\n",
            "bi=100, loss=0.2834688425064087\n",
            "bi=110, loss=0.3369932472705841\n",
            "bi=120, loss=0.3206125795841217\n",
            "bi=130, loss=0.2613537907600403\n",
            "bi=140, loss=0.29763197898864746\n",
            "bi=150, loss=0.2566080093383789\n",
            "bi=160, loss=0.21656261384487152\n",
            "bi=170, loss=0.23266170918941498\n",
            "bi=180, loss=0.23586498200893402\n",
            "bi=190, loss=0.25252076983451843\n",
            "bi=200, loss=0.23837001621723175\n",
            "bi=210, loss=0.1991264522075653\n",
            "bi=220, loss=0.23199643194675446\n",
            "bi=230, loss=0.3661993443965912\n",
            "bi=240, loss=0.14286498725414276\n",
            "bi=250, loss=0.15331502258777618\n",
            "bi=260, loss=0.23158037662506104\n",
            "bi=270, loss=0.17085039615631104\n",
            "bi=280, loss=0.21439027786254883\n",
            "bi=290, loss=0.1919163018465042\n",
            "bi=300, loss=0.33243271708488464\n",
            "bi=310, loss=0.21263250708580017\n",
            "bi=320, loss=0.2751356065273285\n",
            "bi=330, loss=0.21438197791576385\n",
            "bi=340, loss=0.3428580164909363\n",
            "bi=350, loss=0.2574491798877716\n",
            "bi=360, loss=0.1986110359430313\n",
            "bi=370, loss=0.14915554225444794\n",
            "bi=380, loss=0.1728070080280304\n",
            "bi=390, loss=0.31939685344696045\n",
            "bi=400, loss=0.3571699857711792\n",
            "bi=410, loss=0.1276674121618271\n",
            "bi=420, loss=0.22662939131259918\n",
            "bi=430, loss=0.1429044008255005\n",
            "bi=440, loss=0.3019421100616455\n",
            "bi=450, loss=0.19317574799060822\n",
            "bi=460, loss=0.2836497724056244\n",
            "bi=470, loss=0.2499103993177414\n",
            "bi=480, loss=0.1750882863998413\n",
            "bi=490, loss=0.22485403716564178\n",
            "bi=500, loss=0.2670882046222687\n",
            "bi=510, loss=0.21203306317329407\n",
            "bi=520, loss=0.26256802678108215\n",
            "bi=530, loss=0.2101113498210907\n",
            "bi=540, loss=0.20424200594425201\n",
            "bi=550, loss=0.17507725954055786\n",
            "bi=560, loss=0.20463146269321442\n",
            "bi=570, loss=0.1773914247751236\n",
            "bi=580, loss=0.24498958885669708\n",
            "bi=590, loss=0.1883302628993988\n",
            "bi=600, loss=0.24143724143505096\n",
            "bi=610, loss=0.1928531974554062\n",
            "bi=620, loss=0.24146538972854614\n",
            "bi=630, loss=0.32760047912597656\n",
            "bi=640, loss=0.15658684074878693\n",
            "bi=650, loss=0.22536587715148926\n",
            "bi=660, loss=0.1659485399723053\n",
            "bi=670, loss=0.18309423327445984\n",
            "bi=680, loss=0.18757402896881104\n",
            "bi=690, loss=0.2148749679327011\n",
            "bi=700, loss=0.269538015127182\n",
            "bi=710, loss=0.27173754572868347\n",
            "bi=720, loss=0.17064957320690155\n",
            "bi=730, loss=0.2491585612297058\n",
            "bi=740, loss=0.26978668570518494\n",
            "bi=750, loss=0.1810736060142517\n",
            "bi=760, loss=0.2073419690132141\n",
            "bi=770, loss=0.26118817925453186\n",
            "bi=780, loss=0.1870274692773819\n",
            "bi=790, loss=0.342460036277771\n",
            "bi=800, loss=0.15583188831806183\n",
            "bi=810, loss=0.18323242664337158\n",
            "AUC = 0.975353712357217\n",
            "bi=0, loss=0.2463715523481369\n",
            "bi=10, loss=0.1761939525604248\n",
            "bi=20, loss=0.2893091142177582\n",
            "bi=30, loss=0.22526533901691437\n",
            "bi=40, loss=0.12346109747886658\n",
            "bi=50, loss=0.2184300273656845\n",
            "bi=60, loss=0.1757851541042328\n",
            "bi=70, loss=0.2197558879852295\n",
            "bi=80, loss=0.3429639935493469\n",
            "bi=90, loss=0.2712814211845398\n",
            "bi=100, loss=0.21102046966552734\n",
            "bi=110, loss=0.3102231025695801\n",
            "bi=120, loss=0.2655397057533264\n",
            "bi=130, loss=0.2645871341228485\n",
            "bi=140, loss=0.22291219234466553\n",
            "bi=150, loss=0.22377841174602509\n",
            "bi=160, loss=0.17467883229255676\n",
            "bi=170, loss=0.21745453774929047\n",
            "bi=180, loss=0.2048313468694687\n",
            "bi=190, loss=0.22059392929077148\n",
            "bi=200, loss=0.2121766358613968\n",
            "bi=210, loss=0.17676198482513428\n",
            "bi=220, loss=0.22748605906963348\n",
            "bi=230, loss=0.3505447208881378\n",
            "bi=240, loss=0.14045371115207672\n",
            "bi=250, loss=0.16699114441871643\n",
            "bi=260, loss=0.192698672413826\n",
            "bi=270, loss=0.12273316830396652\n",
            "bi=280, loss=0.21861135959625244\n",
            "bi=290, loss=0.17557604610919952\n",
            "bi=300, loss=0.29761818051338196\n",
            "bi=310, loss=0.21935971081256866\n",
            "bi=320, loss=0.27118638157844543\n",
            "bi=330, loss=0.22534224390983582\n",
            "bi=340, loss=0.3152187466621399\n",
            "bi=350, loss=0.23850834369659424\n",
            "bi=360, loss=0.1822272539138794\n",
            "bi=370, loss=0.12566351890563965\n",
            "bi=380, loss=0.14490629732608795\n",
            "bi=390, loss=0.30283308029174805\n",
            "bi=400, loss=0.36261436343193054\n",
            "bi=410, loss=0.11945787817239761\n",
            "bi=420, loss=0.22389215230941772\n",
            "bi=430, loss=0.12967409193515778\n",
            "bi=440, loss=0.2796071171760559\n",
            "bi=450, loss=0.1896367222070694\n",
            "bi=460, loss=0.2669547498226166\n",
            "bi=470, loss=0.20987600088119507\n",
            "bi=480, loss=0.1536474972963333\n",
            "bi=490, loss=0.21290908753871918\n",
            "bi=500, loss=0.2431667000055313\n",
            "bi=510, loss=0.2068578600883484\n",
            "bi=520, loss=0.25721994042396545\n",
            "bi=530, loss=0.19434338808059692\n",
            "bi=540, loss=0.199110209941864\n",
            "bi=550, loss=0.15792961418628693\n",
            "bi=560, loss=0.1885596662759781\n",
            "bi=570, loss=0.1682395040988922\n",
            "bi=580, loss=0.2214449942111969\n",
            "bi=590, loss=0.17019356787204742\n",
            "bi=600, loss=0.22760240733623505\n",
            "bi=610, loss=0.19220399856567383\n",
            "bi=620, loss=0.22264109551906586\n",
            "bi=630, loss=0.24429026246070862\n",
            "bi=640, loss=0.13925102353096008\n",
            "bi=650, loss=0.22211232781410217\n",
            "bi=660, loss=0.1492147147655487\n",
            "bi=670, loss=0.17442665994167328\n",
            "bi=680, loss=0.19422316551208496\n",
            "bi=690, loss=0.2080841362476349\n",
            "bi=700, loss=0.2609456777572632\n",
            "bi=710, loss=0.24940621852874756\n",
            "bi=720, loss=0.18646101653575897\n",
            "bi=730, loss=0.19094249606132507\n",
            "bi=740, loss=0.25027987360954285\n",
            "bi=750, loss=0.1854744553565979\n",
            "bi=760, loss=0.1983747035264969\n",
            "bi=770, loss=0.234849214553833\n",
            "bi=780, loss=0.17434781789779663\n",
            "bi=790, loss=0.26744958758354187\n",
            "bi=800, loss=0.16641302406787872\n",
            "bi=810, loss=0.16383230686187744\n",
            "AUC = 0.992219950674974\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}